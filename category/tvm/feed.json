{
    "version": "https://jsonfeed.org/version/1",
    "title": "MikeMao's blog • All posts by \"tvm\" category",
    "description": "a student of NJU who use this website to record learning experience",
    "home_page_url": "http://example.com",
    "items": [
        {
            "id": "http://example.com/2023/02/28/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BC%96%E8%AF%91%E5%99%A8TVM-UserTutorial/",
            "url": "http://example.com/2023/02/28/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BC%96%E8%AF%91%E5%99%A8TVM-UserTutorial/",
            "title": "深度学习编译器TVM-UserTutorial",
            "date_published": "2023-02-28T06:03:36.000Z",
            "content_html": "<h1 id=\"tvm-usertutorial\"><a class=\"anchor\" href=\"#tvm-usertutorial\">#</a> TVM-UserTutorial</h1>\n<p>自 2022 年 10 月份选择大创项目 —— 基于 TVM 实现针对国产 AI 芯片的深度算子库项目以来。花了许多时间弄懂什么是 TVM，项目的定位是什么，我们要做的到底是什么事。也经历了期末复习周，由于疫情延期期末考到开学考试，以及小组成员陆续阳性等等事。导致我们的项目在开题答辩之后迟迟没有开始。本学期初在学习了计算系统基础以及编译原理的导论之后，我对 TVM 的认识更加深刻了，项目也重新启动 (2023/2)。</p>\n<p><span id=\"more\"></span></p>\n<h2 id=\"introduction\"><a class=\"anchor\" href=\"#introduction\">#</a> Introduction</h2>\n<p>原文链接： <span class=\"exturl\" data-url=\"aHR0cHM6Ly90dm0uYXBhY2hlLm9yZy9kb2NzL3R1dG9yaWFsL2ludHJvZHVjdGlvbi5odG1sI3NwaHgtZ2xyLXR1dG9yaWFsLWludHJvZHVjdGlvbi1weQ==\">Introduction — tvm 0.11.dev0 documentation (apache.org)</span></p>\n<p>TVM 隶属于 Apache 基金会，是开源项目。定义为一个可应用于各种 GPU CPU 深度学习加速器的深度学习编译器。</p>\n<h2 id=\"an-overview-of-tvm-and-model-optimization\"><a class=\"anchor\" href=\"#an-overview-of-tvm-and-model-optimization\">#</a> An Overview of TVM and Model Optimization</h2>\n<p><img data-src=\"image-20230228142729726.png\" alt=\"image-20230228142729726\"></p>\n<p>主要思想： 专用 --&gt; 通用 --&gt; 专用    不同框架转化为统一的 IR 表示，在根据模型所部署的不同硬件的类型转化为对应的字节码 。</p>\n<p>TVM 采用了多级 IR 的设计，而且每级之间的转化都会经过各种优化，比如切割子图，图优化，以及 AutoTVM/AutoScheduler (这两个是 TVM 带有的自动优化模块) 进行最优调度的选择。</p>\n<p>TVM 支持的后端有 LLVM、NVCC 等，最重要的是可以支持 Embedded and specialized targets, 但是要使用 TVM 提供的 BYOC 功能，也是我们项目最需要实现的部分。</p>\n<h2 id=\"compiling-and-optimizing-a-model-with-tvmc\"><a class=\"anchor\" href=\"#compiling-and-optimizing-a-model-with-tvmc\">#</a> compiling and optimizing a Model with TVMC</h2>\n<p>原文链接：<span class=\"exturl\" data-url=\"aHR0cHM6Ly90dm0uYXBhY2hlLm9yZy9kb2NzL3R1dG9yaWFsL3R2bWNfY29tbWFuZF9saW5lX2RyaXZlci5odG1sI3NwaHgtZ2xyLXR1dG9yaWFsLXR2bWMtY29tbWFuZC1saW5lLWRyaXZlci1weQ==\">compiling and optimizing a model with TVMC</span></p>\n<p>大部分操作已在虚拟机中运行成功</p>\n<p>日期：3/1</p>\n<p>这章主要熟悉了 TVMC 的简单使用。官方文档提供了一个预训练的 ResNet-50 v2 模型。为 TVM 的 runtime 编译此模型，并在这个模型上跑了一个真实的猫猫图片，得到运行结果。文章还包括了在实际的 CPU 上用 TVM 调优（tune）模型，并且使用 TVM 收集的 tuning data 重编译出一个优化的模型，重新跑一遍优化的模型，并与之前模型的表现进行对比（这部分较难）。</p>\n<p>TVMC 是 TVM 的命令行工具，让你能在命令行中使用 TVM，C 表示 command line 的意思。</p>\n<p>TVMC 支持 Keras, ONNX, TensorFlow, TFLite and Torch 构建的模型，本章模型用的是 onnx。</p>\n<h2 id=\"编译模型\"><a class=\"anchor\" href=\"#编译模型\">#</a> 编译模型</h2>\n<figure class=\"highlight bash\"><figcaption data-lang=\"bash\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre>tvmc compile <span class=\"token punctuation\">\\</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre><span class=\"token parameter variable\">--target</span> <span class=\"token string\">\"llvm\"</span> <span class=\"token punctuation\">\\</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre>--input-shapes <span class=\"token string\">\"data:[1,3,224,224]\"</span> <span class=\"token punctuation\">\\</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre><span class=\"token parameter variable\">--output</span> resnet50-v2-7-tvm.tar <span class=\"token punctuation\">\\</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre>resnet50-v2-7.onnx</pre></td></tr><tr><td data-num=\"6\"></td><td><pre><span class=\"token comment\"># 编译出来一个 tar 文件</span></pre></td></tr><tr><td data-num=\"7\"></td><td><pre><span class=\"token function\">mkdir</span> model</pre></td></tr><tr><td data-num=\"8\"></td><td><pre><span class=\"token function\">tar</span> <span class=\"token parameter variable\">-xvf</span> resnet50-v2-7-tvm.tar <span class=\"token parameter variable\">-C</span> model</pre></td></tr><tr><td data-num=\"9\"></td><td><pre><span class=\"token function\">ls</span> model</pre></td></tr><tr><td data-num=\"10\"></td><td><pre><span class=\"token comment\">#解压后看看有什么</span></pre></td></tr></table></figure><p>有三个东西：</p>\n<p><span class=\"exturl\" data-url=\"aHR0cDovL21vZC5zbw==\">mod.so</span> : 就是那个模型，表现为一个 TVM runtime 能运行的 c++ 库</p>\n<p>mod.json  : a text representation of the TVM Relay computation graph.</p>\n<p>mod.params:  a file containing the parameters for the pre-trained model.--</p>\n<p>编译时选择恰当的命令行选项能大大改变编译性能</p>\n<h2 id=\"用tvmc运行模型\"><a class=\"anchor\" href=\"#用tvmc运行模型\">#</a> 用 TVMC 运行模型</h2>\n<p>为了对模型进行有效输入 要将预备的输入进行一些预处理.TVM 采用.npz 格式文件作为模型输入和输出，这是一个受良好支持的 Numpy 文件格式。</p>\n<p>这里文章用一个 python 脚本预处理了猫猫图片（具体见原文），得到了一个 imagenet_cat.npz。</p>\n<figure class=\"highlight bash\"><figcaption data-lang=\"bash\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre>tvmc run <span class=\"token punctuation\">\\</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre><span class=\"token parameter variable\">--inputs</span> imagenet_cat.npz <span class=\"token punctuation\">\\</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre><span class=\"token parameter variable\">--output</span> predictions.npz <span class=\"token punctuation\">\\</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>resnet50-v2-7-tvm.tar</pre></td></tr></table></figure><p>同样，这个文件也不是人能看懂的，原文提供了一个后处理脚本，把这个文件转化为人能看懂的结果。（见原文）</p>\n<p>结果如下：</p>\n<blockquote>\n<h4 id=\"classn02123045-tabby-tabby-cat-with-probability0610553\"><a class=\"anchor\" href=\"#classn02123045-tabby-tabby-cat-with-probability0610553\">#</a> class='n02123045 tabby, tabby cat' with probability=0.610553</h4>\n<h4 id=\"classn02123159-tiger-cat-with-probability0367179\"><a class=\"anchor\" href=\"#classn02123159-tiger-cat-with-probability0367179\">#</a> class='n02123159 tiger cat' with probability=0.367179</h4>\n<h4 id=\"classn02124075-egyptian-cat-with-probability0019365\"><a class=\"anchor\" href=\"#classn02124075-egyptian-cat-with-probability0019365\">#</a> class='n02124075 Egyptian cat' with probability=0.019365</h4>\n<h4 id=\"classn02129604-tiger-panthera-tigris-with-probability0001273\"><a class=\"anchor\" href=\"#classn02129604-tiger-panthera-tigris-with-probability0001273\">#</a> class='n02129604 tiger, Panthera tigris' with probability=0.001273</h4>\n<h4 id=\"classn04040759-radiator-with-probability0000261\"><a class=\"anchor\" href=\"#classn04040759-radiator-with-probability0000261\">#</a> class='n04040759 radiator' with probability=0.000261</h4>\n</blockquote>\n<h2 id=\"自动调优模型\"><a class=\"anchor\" href=\"#自动调优模型\">#</a> 自动调优模型</h2>\n<p>TVM 的调优是根据具体硬件对模型进行优化，使其在给定目标上运行的更快。调优不会影响预测准确性，只会影响性能！</p>\n<p>演示：</p>\n<figure class=\"highlight bash\"><figcaption data-lang=\"bash\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre>tvmc tune <span class=\"token punctuation\">\\</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre><span class=\"token parameter variable\">--target</span> <span class=\"token string\">\"llvm\"</span> <span class=\"token punctuation\">\\</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre><span class=\"token parameter variable\">--output</span> resnet50-v2-7-autotuner_records.json <span class=\"token punctuation\">\\</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>resnet50-v2-7.onnx</pre></td></tr></table></figure><p>如果为 ——target 指定一个更具体的目标能得到更好的结果，如在 i7 处理器上使用 --target llvm-mcpu=skylake</p>\n<p>TVMC 将对模型的参数空间进行搜索，尝试不同的算子配置，并选择在您的平台上运行最快的配置。虽然这是一个基于 CPU 和模型运算的引导搜索，但仍然需要几个小时才能完成搜索。此搜索的输出将保存到 resnet50-v2-7-autotuner_records.json 文件中，稍后将用于编译一个优化的模型。</p>\n<h2 id=\"使用调优数据编译优化模型\"><a class=\"anchor\" href=\"#使用调优数据编译优化模型\">#</a> 使用调优数据编译优化模型</h2>\n<p>编译器将使用调优结果为指定的目标上的模型生成高性能代码。编译命令为 tvmc compile --tuning-records。现在已经收集了模型的调优数据，我们可以使用优化后的算子重新编译模型，以加快计算速度。</p>\n<figure class=\"highlight bash\"><figcaption data-lang=\"bash\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre>tvmc compile <span class=\"token punctuation\">\\</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre><span class=\"token parameter variable\">--target</span> <span class=\"token string\">\"llvm\"</span> <span class=\"token punctuation\">\\</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre>--tuning-records resnet50-v2-7-autotuner_records.json  <span class=\"token punctuation\">\\</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre><span class=\"token parameter variable\">--output</span> resnet50-v2-7-tvm_autotuned.tar <span class=\"token punctuation\">\\</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre>resnet50-v2-7.onnx</pre></td></tr></table></figure><p>与之前作对比，重复 100 此次。平均性能快 47%。</p>\n",
            "tags": [
                "TVM 深度学习 机器学习 编译"
            ]
        }
    ]
}