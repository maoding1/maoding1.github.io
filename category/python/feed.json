{
    "version": "https://jsonfeed.org/version/1",
    "title": "MikeMao's blog • All posts by \"python\" category",
    "description": "a student of NJU who use this website to record learning experience",
    "home_page_url": "http://example.com",
    "items": [
        {
            "id": "http://example.com/2023/03/16/nova%E2%80%94%E2%80%94%E9%AB%98%E8%80%83%E7%BD%91%E7%AB%99%E5%AD%A6%E6%A0%A1%E5%90%8D%E7%A7%B0%E7%88%AC%E5%8F%96/",
            "url": "http://example.com/2023/03/16/nova%E2%80%94%E2%80%94%E9%AB%98%E8%80%83%E7%BD%91%E7%AB%99%E5%AD%A6%E6%A0%A1%E5%90%8D%E7%A7%B0%E7%88%AC%E5%8F%96/",
            "title": "nova——高考网站学校名称爬取",
            "date_published": "2023-03-16T12:06:14.000Z",
            "content_html": "<h1 id=\"nova项目-高考网站学校名称爬取\"><a class=\"anchor\" href=\"#nova项目-高考网站学校名称爬取\">#</a> nova 项目 - 高考网站学校名称爬取</h1>\n<p>任务：在高考特殊类型招生报名网站中爬取全中国的中学名称</p>\n<p>但是遭遇了强大的反爬措施 遂把失败经历记录如下</p>\n<p><span id=\"more\"></span></p>\n<p>首先使用 selenium 不加任何反爬措施登录网站试试：</p>\n<p>发现第一关就过不了</p>\n<p><img data-src=\"image-20230318120750666.png\" alt=\"image-20230318120750666\"></p>\n<p>请求显示 400 bad request 表明我们的请求被识别为非法的请求</p>\n<p>在控制台输入  <code>window.navigator.webdriver</code></p>\n<p>显示结果：</p>\n<p><img data-src=\"image-20230318120845415.png\" alt=\"image-20230318120845415\"></p>\n<p>间接地表示了你使用了 selenium 等自动化工具</p>\n<h2 id=\"尝试方案一加入配置\"><a class=\"anchor\" href=\"#尝试方案一加入配置\">#</a> 尝试方案一 (加入配置)：</h2>\n<p>在 stackoverflow 搜索 selenium 400 bad request</p>\n<p>有人提供了解决方案 :<img data-src=\"image-20230318122455280.png\" alt=\"image-20230318122455280\"></p>\n<p>在代码中加入：</p>\n<figure class=\"highlight python\"><figcaption data-lang=\"python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre>edge_options<span class=\"token punctuation\">.</span>add_argument<span class=\"token punctuation\">(</span><span class=\"token string\">\"--disable-blink-features=AutomationControlled\"</span><span class=\"token punctuation\">)</span></pre></td></tr></table></figure><p>这个配置可以去除 webdriver 的一些特征 但是在这个网站中还是失败</p>\n<p>在控制台输入  <code>window.navigator.webdriver</code>   仍然显示 true，说明特征没隐藏完</p>\n<h2 id=\"尝试方案二注入js代码\"><a class=\"anchor\" href=\"#尝试方案二注入js代码\">#</a> 尝试方案二 (注入 JS 代码)：</h2>\n<p>加入代码如下：</p>\n<figure class=\"highlight python\"><figcaption data-lang=\"python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre>driver<span class=\"token punctuation\">.</span>execute_cdp_cmd<span class=\"token punctuation\">(</span><span class=\"token string\">\"Page.addScriptToEvaluateOnNewDocument\"</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre>    <span class=\"token string\">\"source\"</span><span class=\"token punctuation\">:</span> <span class=\"token triple-quoted-string string\">\"\"\"</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre>    Object.defineProperty(navigator, 'webdriver', &#123;</pre></td></tr><tr><td data-num=\"4\"></td><td><pre>      get: () => false</pre></td></tr><tr><td data-num=\"5\"></td><td><pre>    &#125;)</pre></td></tr><tr><td data-num=\"6\"></td><td><pre>  \"\"\"</pre></td></tr><tr><td data-num=\"7\"></td><td><pre><span class=\"token punctuation\">&#125;</span><span class=\"token punctuation\">)</span></pre></td></tr></table></figure><ul>\n<li>\n<p>cdp (Chrome Devtools Protocol) 命令 通过 CDP, 可以检查 / 调试 / 监听网络流量，</p>\n</li>\n<li>\n<p>Page.addScriptToEvaluateOnNewDocument：在浏览器启动之前执行给定的 JS 脚本</p>\n</li>\n<li>\n<p>Object.defineProperty：JS 语法，直接在一个对象上定义一个新属性，或者修改一个对象的现有属性，并返回此对象（代码中是将  <code>navigator.webdriver</code>  设置为  <code>false</code> ）</p>\n</li>\n</ul>\n<p>结果如下： 可以看到 webdriver 的一些特征确实被隐藏了</p>\n<p><img data-src=\"image-20230318124126335.png\" alt=\"image-20230318124126335\"></p>\n<p>但是页面仍然一片空白 请求包仍然是 400 bad request</p>\n<h2 id=\"思考一\"><a class=\"anchor\" href=\"#思考一\">#</a> 思考一：</h2>\n<p>上面的方法都在尝试隐藏某些你在使用爬虫的痕迹 部分网站检测不会那么深 但是使用爬虫总会留下痕迹</p>\n<p>selenium 启动的浏览器 ，有几十个特征可以被检测到</p>\n<p>有一个网站：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9ib3Quc2Fubnlzb2Z0LmNvbS8=\">https://bot.sannysoft.com/</span> 可以检测你的浏览指纹</p>\n<p>使用无头浏览器直接打开这个网站并截图：</p>\n<p><img data-src=\"image-20230318130242034.png\" alt=\"image-20230318130242034\"></p>\n<p>标红的都是未通过的测试</p>\n<p>用刚刚尝试的方法 get 这个网站 好了很多 但仍显示未通过爬虫检测</p>\n<p><img data-src=\"image-20230318130449697.png\" alt></p>\n<p>正常浏览器打开：</p>\n<p><img data-src=\"image-20230318125152944.png\" alt></p>\n<h2 id=\"尝试方案三更强大的js注入\"><a class=\"anchor\" href=\"#尝试方案三更强大的js注入\">#</a> 尝试方案三 (更强大的 JS 注入):</h2>\n<p>方案二的 js 代码还是太简单了，只能隐藏一个特定的特征（虽然这个特征比较常用）</p>\n<p>在网上看看有没有更专业的开发者开发出来的更强大的 js 代码：</p>\n<p>找到了一份  <strong>stealth.min.js</strong></p>\n<p>那么，这个 <code>stealth.min.js</code>  文件是怎么来的呢？这就要说到 <code>pyppeteer</code>  (一个与 selenium 同类的爬虫库) 了。，Python 版本的 <code>pyppeteer</code>  已经很久没有人维护了，但是 Node.js 版本的  <code>puppeteer</code>  持续有人维护，并且在持续更新，生态也越来越好。</p>\n<p>有开发者给 puppeteer 写了一套插件，叫做 <code>puppeteer-extra</code> 。其中，就有一个插件叫做<strong> puppeteer-extra-plugin-stealth</strong>。这个东西，就来专门用来让 puppeteer 隐藏模拟浏览器的指纹特征。</p>\n<p>那么，我们用 Python  selenium 的人怎么办呢？实际上也有办法。就是把其中的隐藏特征的脚本提取出来，做成一个单独的 js 文件。然后让 Selenium 在打开任意网页之前，先运行一下这个 js 文件里面的内容。</p>\n<p>puppeteer-extra-plugin-stealth 的作者还写了另外一个工具，叫做<strong> extract-stealth-evasions</strong>。这个东西就是用来生成 <code>stealth.min.js</code>  文件的。</p>\n<p>首先安装 Node.js，然后安装 Npm，接着运行如下命令：</p>\n<p><code>npx extract-stealth-evasions </code></p>\n<p>就能在文件夹下生成一个 stealth.min.js 文件了</p>\n<p>使用 stealth.min.js 如下：</p>\n<figure class=\"highlight python\"><figcaption data-lang=\"python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token keyword\">with</span> <span class=\"token builtin\">open</span><span class=\"token punctuation\">(</span><span class=\"token string\">'stealth.min.js'</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">as</span> f<span class=\"token punctuation\">:</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre>    js <span class=\"token operator\">=</span> f<span class=\"token punctuation\">.</span>read<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre>driver<span class=\"token punctuation\">.</span>execute_cdp_cmd<span class=\"token punctuation\">(</span><span class=\"token string\">\"Page.addScriptToEvaluateOnNewDocument\"</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">&#123;</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>    <span class=\"token string\">\"source\"</span><span class=\"token punctuation\">:</span> js</pre></td></tr><tr><td data-num=\"5\"></td><td><pre><span class=\"token punctuation\">&#125;</span><span class=\"token punctuation\">)</span></pre></td></tr></table></figure><p>再登录刚才的网站：</p>\n<p><img data-src=\"image-20230318132200964.png\" alt></p>\n<p>通过此网站的测试</p>\n<p>再尝试我们要爬取的网站 请求包仍然是 400 bad request</p>\n<p>猜测失败原因是反爬措施太强大了 不管怎么做都会留下使用爬虫的痕迹</p>\n<h2 id=\"尝试方案四连接手动打开的浏览器\"><a class=\"anchor\" href=\"#尝试方案四连接手动打开的浏览器\">#</a> 尝试方案四（连接手动打开的浏览器）：</h2>\n<p>既然使用驱动程序的 selenium 总会留下特征，手动打开的浏览器不会被检测到，那我能不能从源头解决问题，先手动打开一个浏览器，然后尝试用 selenium 接管它呢。</p>\n<figure class=\"highlight python\"><figcaption data-lang=\"python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre>cmd1 <span class=\"token operator\">=</span> <span class=\"token string\">'cd \"C:\\Program Files (x86)\\Microsoft\\Edge\\Application\" &amp;&amp; start .\\msedge.exe --remote-debugging-port=9225 '</span> \\</pre></td></tr><tr><td data-num=\"2\"></td><td><pre>           <span class=\"token string\">'--user-data-dir=\"C:\\selenium\\EdgeProfile\" '</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre>    os<span class=\"token punctuation\">.</span>system<span class=\"token punctuation\">(</span>cmd1<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>    edge_options <span class=\"token operator\">=</span> Options<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre>    edge_options<span class=\"token punctuation\">.</span>add_experimental_option<span class=\"token punctuation\">(</span><span class=\"token string\">\"debuggerAddress\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"127.0.0.1:9225\"</span><span class=\"token punctuation\">)</span></pre></td></tr></table></figure><p>上面代码在命令行中用调试模式打开了 Edge 浏览器，并且开放了一个远程调试端口，我们再使用 selenium 连接上这个端口，取得控制权。</p>\n<p>在 get 一下目标网址，并登录：</p>\n<p><img data-src=\"image-20230318134112693.png\" alt></p>\n<p>终于能进来了。</p>\n<p>要爬取的中学名称信息分为四个下拉列表：</p>\n<p><img data-src=\"image-20230318134447273.png\" alt=\"image-20230318134447273\"></p>\n<p>当选中了上一级下拉列表的某个选项后，服务端会返回一个 ajax 包，并更新下一级下拉列表里的内容，在这里仍然遇到问题。</p>\n<p>如果用 selenium 选择了某一个选项，发现后面的下拉列表并没有更新。</p>\n<p>演示一下。</p>\n<p>通过检查工具抓包，发现还是 400 bad request，猜测爬虫还是被检测出来了。</p>\n<h2 id=\"尝试方案五模拟真实环境\"><a class=\"anchor\" href=\"#尝试方案五模拟真实环境\">#</a> 尝试方案五 (模拟真实环境)</h2>\n<p>猜测：爬虫遍历列表选项并点击的频率太快了，被识别出来非人工操作</p>\n<p>在每次点击选项后都加上 time.sleep (1) 休眠一秒钟，仍然失败</p>\n<h2 id=\"尝试方案六直接对ajax包的url发请求\"><a class=\"anchor\" href=\"#尝试方案六直接对ajax包的url发请求\">#</a> 尝试方案六 (直接对 ajax 包的 url 发请求)</h2>\n<p>一个 url 示例：</p>\n<p><strong><span class=\"exturl\" data-url=\"aHR0cHM6Ly9nYW9rYW8uY2hzaS5jb20uY24venpibS9xdWVyeVhqcy5hY3Rpb24/TFVWVndzODA9NUgyX1l0S2NmN0FCQXZhV00xVThCWjJJNFFWWHUuWlhuQ0RITUVsR0JKY0Vma3dxY0dWNDlIUzQ2d2FTOWQ5ZXk3eUlTZG5BOG95WUh6MkxTVkdJYnVHVEpRVXpBWjNaRUt0d3pKZ0NtSm5oMDNjNlhteVA2ZnVwdXVTUTN6elVOdlFUUndTX1cxeTZtY055TEhDdl82czhIX1IzY2xRYUk3czN4OG5yNFg3NVdDUUFCbFRTWURTMnhtU3p6MnczQ21fMDh6bDJ3Ymd1aTR2NjVCb2laMkw2c0hEYWdSWl95d3ZtTmpfLlg1SDlhbTYwYlpNMGpZaEtwQ19Fck9RcUxfdjhWRmgyN0plQ1g5YldSNW9JTTlRSjEyNldqTUdmM1NldzJTNjhzeFhUVHdmWG9NcTVULjE3WXdrbjRwZGlralZsUWhDc25jbGV4Z3cwaW94dVVjMTMwVEhVdFVQaEQudGx0M2pKYnp5WnpfU1pJLlhyTVJuU3ZWamdpcHBvWg==\">https://gaokao.chsi.com.cn/zzbm/queryXjs.action?LUVVws80=5H2_YtKcf7ABAvaWM1U8BZ2I4QVXu.ZXnCDHMElGBJcEfkwqcGV49HS46waS9d9ey7yISdnA8oyYHz2LSVGIbuGTJQUzAZ3ZEKtwzJgCmJnh03c6XmyP6fupuuSQ3zzUNvQTRwS_W1y6mcNyLHCv_6s8H_R3clQaI7s3x8nr4X75WCQABlTSYDS2xmSzz2w3Cm_08zl2wbgui4v65BoiZ2L6sHDagRZ_ywvmNj_.X5H9am60bZM0jYhKpC_ErOQqL_v8VFh27JeCX9bWR5oIM9QJ126WjMGf3Sew2S68sxXTTwfXoMq5T.17Ywkn4pdikjVlQhCsnclexgw0ioxuUc130THUtUPhD.tlt3jJbzyZz_SZI.XrMRnSvVjgippoZ</span></strong></p>\n<p>对要携带的参数的 key 值和 value 进行了加密 找不到规律 也失败</p>\n<h2 id=\"尝试方案七undetected_driver\"><a class=\"anchor\" href=\"#尝试方案七undetected_driver\">#</a> 尝试方案七 (undetected_driver)</h2>\n<p>向老师求助后得到的方案</p>\n<p>undetected_driver 也是 selenium 提供的一个模块 可以防止浏览器特征被识别，并且可以根据浏览器版本自动下载驱动。</p>\n<p>可以过登录 但是发来的 ajax 请求包还是 400 bad request</p>\n<h2 id=\"总结\"><a class=\"anchor\" href=\"#总结\">#</a> 总结</h2>\n<p>猜测失败原因：还是有没有隐藏好的特征被反爬措施检查到了。</p>\n<p>查找替代方案：在搜索引擎搜索全国中学名称，有结果，但是数据比较老，而且权威性无法保证。在中国教育部官网搜索没有结果。可能需要在同类的政府网站填写个人信息的时候可以找到。</p>\n",
            "tags": [
                "python 爬虫 计算机网络"
            ]
        },
        {
            "id": "http://example.com/2023/01/25/python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0/",
            "url": "http://example.com/2023/01/25/python%E7%88%AC%E8%99%AB%E5%AD%A6%E4%B9%A0/",
            "title": "python爬虫学习",
            "date_published": "2023-01-25T03:21:52.000Z",
            "content_html": "<h1 id=\"python爬虫学习总结\"><a class=\"anchor\" href=\"#python爬虫学习总结\">#</a> python 爬虫学习总结</h1>\n<p>学习 python 爬虫的一些笔记</p>\n<p><span id=\"more\"></span></p>\n<h2 id=\"requests模块\"><a class=\"anchor\" href=\"#requests模块\">#</a> requests 模块</h2>\n<p>requests 模块流程：</p>\n<pre><code>1. 指定url\n2. 发起请求 response = requests.get(url)\n3. 获取数据 page_source = response.text()\n4. 存储数据 文件操作/数据库操作\n</code></pre>\n<p>参数 tips：params 是用来发送查询字符串，而 data 是用来发送正文的。post 与 get 方法的特性是：这两种参数 post（）方法都可以用，get 方法只能发查询字符串，不能发送正文</p>\n<p>text 返回字符串 content 返回二进制（爬取图片可用）  json 返回对象</p>\n<p>分页爬取：</p>\n<p>​\t如果是 ajax，在检查、网络、xhr 中查看</p>\n<p>​\t如果是新链接，可以使用正则表达式匹配 url</p>\n<p>如果是用 ajax 的包：要注意的：</p>\n<ul>\n<li>看请求方法 是 GET 还是 POST。。。</li>\n<li>看响应头的 content-type 选择 reponse.text () 或 reponse.json ()</li>\n<li>看负载中带的参数</li>\n</ul>\n<p>中文乱码问题（文件编码格式为 utf8 json.dump 时 ensure acill 选项关闭）</p>\n<p>什么是 ajax? （根据用户行为重新渲染界面。 不修改 url 在检查 - 网络 - XHR 中查看具体信息）</p>\n<h2 id=\"bs4-数据解析\"><a class=\"anchor\" href=\"#bs4-数据解析\">#</a> bs4 数据解析</h2>\n<p>bs4 数据解析的原理：</p>\n<p>1. 实例化一个 BeautifulSoup 对象 并且将页面原码数据加载到该对象中</p>\n<p>2. 通过调用 bs 对象中相关的属性或方法进行标签定位和数据提取</p>\n<p>如何实例化 beautifulsoup 对象：</p>\n<ol>\n<li>\n<p>将本地的 html 文档中的数据加载到该对象中</p>\n<figure class=\"highlight python\"><figcaption data-lang=\"python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre>fp <span class=\"token operator\">=</span> <span class=\"token builtin\">open</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre>soup <span class=\"token operator\">=</span> BeautifulSoup<span class=\"token punctuation\">(</span>fp<span class=\"token punctuation\">,</span><span class=\"token string\">'lxml'</span><span class=\"token punctuation\">)</span></pre></td></tr></table></figure></li>\n<li>\n<p>将互联网上获取的页面源码加载到该对象中</p>\n<pre><code>page_text = response.text\nsoup = BeautifulSoup(page_text,'lxml')\n</code></pre>\n</li>\n</ol>\n<p>提供的用于数据解析的方法和属性：</p>\n<p>soup.tagname 返回 html 中第一次出现的 tagname 标签</p>\n<p>soup.find ('tagname') 等同于 soup.tagname</p>\n<p>soup.find ('tagname', class = 'classname') 定义属性位置， class 可以替换为 id attr</p>\n<p>soup.find_all ('tagname') 找到符合要求所有标签 返回列表</p>\n<p>soup.select ('.tang')  参数为某种选择器（id class 标签），返回一个列表</p>\n<p>soup.select ('.tang&gt; ul &gt; li &gt; a') 层级选择器，返回一个列表</p>\n<p>soup.selest ('.tang&gt; ul a') &gt; 号表示一个层级，空格表示多个层级</p>\n<p>怎么获取标签之间的文本数据：</p>\n<p>使用\tsoup.a.text/string/get_text () 方法</p>\n<pre><code>-   text/get_text(): 可以获取某一个标签中所有的文本内容（多套几层也能得到）\n-   string：获取标签下直系的文本内容\n</code></pre>\n<p>怎么获取标签的属性值：</p>\n<p><code>soup.select('tang &gt; ul &gt; li &gt; a')[0]['href']</code>  直接获得 a 标签中的 herf 属性值</p>\n<h2 id=\"xpath解析\"><a class=\"anchor\" href=\"#xpath解析\">#</a> xpath 解析</h2>\n<p>xpath 解析： 最常用且最便捷高效的一种解析方式。</p>\n<p>xpath 解析原理：</p>\n<pre><code>1. 实例化一个etree的对象，且需要将被解析的页面源码数据加载到该对象中。\n2. 调用etree对象中的xpath方法结合xpath表达式实现标签的定位和内容的捕获。\n</code></pre>\n<p>环境安装：</p>\n<pre><code>- pip install lxml\n</code></pre>\n<p>如何实例化 etree 对象</p>\n<p>​\tfrom lxml import etree</p>\n<ol>\n<li>\n<p>将本地的 html 文档中的源码数据加载到 etree 对象中：</p>\n<p><code> etree.parse(filePath)</code></p>\n</li>\n<li>\n<p>可以将从互联网上获取的源码数据加载到该对象中</p>\n<p><code>etree.HTML('page_text')        -xpath('xpath表达式')</code></p>\n</li>\n</ol>\n<p>/ 表示一个层级  // 表示多个层级</p>\n<p>路径后加 [@ 属性名] 精确定位 如 <code>xx_list = tree.xpath('//div[@class='song']')</code></p>\n<p>中括号内加数字代表取第几个（索引下标从 1 开始） <code>xx_list[0]</code></p>\n<p>获取文本：定位后加 /text ()</p>\n<h2 id=\"中文出现乱码常见的处理方法\"><a class=\"anchor\" href=\"#中文出现乱码常见的处理方法\">#</a> 中文出现乱码常见的处理方法</h2>\n<p>两种常见方法，一般能解决所有中文乱码问题</p>\n<ol>\n<li>\n<p>对爬下来的整个数据改变编码：</p>\n<figure class=\"highlight python\"><figcaption data-lang=\"python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre>response<span class=\"token punctuation\">.</span>encoding <span class=\"token operator\">=</span> <span class=\"token string\">'utf-8'</span></pre></td></tr></table></figure></li>\n<li>\n<p>对出现问题的对象单独进行编码与解码</p>\n<figure class=\"highlight python\"><figcaption data-lang=\"python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre>img_name <span class=\"token operator\">=</span> img_name<span class=\"token punctuation\">.</span>encode<span class=\"token punctuation\">(</span><span class=\"token string\">'iso-8859-1'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>decode<span class=\"token punctuation\">(</span><span class=\"token string\">'gbk'</span><span class=\"token punctuation\">)</span></pre></td></tr></table></figure></li>\n</ol>\n<h2 id=\"selenium模块\"><a class=\"anchor\" href=\"#selenium模块\">#</a> selenium 模块</h2>\n<h3 id=\"selenium模块与爬虫之间具有怎样的关联\"><a class=\"anchor\" href=\"#selenium模块与爬虫之间具有怎样的关联\">#</a> selenium 模块与爬虫之间具有怎样的关联？</h3>\n<ul>\n<li>便捷的获取网站中动态加载的数据</li>\n<li>便捷实现模拟登录</li>\n</ul>\n<h3 id=\"什么是selenium模块\"><a class=\"anchor\" href=\"#什么是selenium模块\">#</a> 什么是 selenium 模块</h3>\n<p>基于浏览器自动化的一个模块。 （编写代码让浏览器完成自动化操作）</p>\n<h3 id=\"selenium使用流程\"><a class=\"anchor\" href=\"#selenium使用流程\">#</a> selenium 使用流程：</h3>\n<ul>\n<li>环境安装：pip install selenium</li>\n<li>下载一个浏览器的驱动程序</li>\n<li>实例化一个浏览器对象</li>\n</ul>\n<figure class=\"highlight python\"><figcaption data-lang=\"python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token keyword\">from</span> selenium <span class=\"token keyword\">import</span> webdriver</pre></td></tr><tr><td data-num=\"2\"></td><td><pre><span class=\"token keyword\">from</span> selenium<span class=\"token punctuation\">.</span>webdriver<span class=\"token punctuation\">.</span>edge<span class=\"token punctuation\">.</span>service <span class=\"token keyword\">import</span> Service</pre></td></tr><tr><td data-num=\"3\"></td><td><pre><span class=\"token comment\"># 实例化一个浏览器对象 参数中的路径为浏览器驱动程序的路径（建议与程序在同一路径下）</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>service <span class=\"token operator\">=</span> Service<span class=\"token punctuation\">(</span>executable_path <span class=\"token operator\">=</span> <span class=\"token string\">'./edgedriver.exe'</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre>bro <span class=\"token operator\">=</span> webdriver<span class=\"token punctuation\">.</span>Edge<span class=\"token punctuation\">(</span>service <span class=\"token operator\">=</span> service<span class=\"token punctuation\">)</span></pre></td></tr></table></figure><ul>\n<li>编写基于浏览器的代码</li>\n</ul>\n<h3 id=\"selenium常用函数\"><a class=\"anchor\" href=\"#selenium常用函数\">#</a> selenium 常用函数</h3>\n<ul>\n<li>\n<p>bro.get (url)  \t\t\t\t\t\t\t\t\t\t\t让浏览器发起一个指定 url 对应请求</p>\n</li>\n<li>\n<p>page_text = bro.page_source\t\t  获取浏览器当前页面的页面源码数据</p>\n</li>\n<li>\n<p>标签定位：使用 find_element 方法</p>\n</li>\n</ul>\n<p>​\tsearch_input = bro.find_element(By.[attrname], &quot;[attr]'s value&quot;)</p>\n<ul>\n<li>\n<p>标签交互：</p>\n<p>​\tsearch_input.send_keys('xxx')</p>\n</li>\n<li>\n<p>执行一组 js 代码</p>\n<p>​\tbro.execute_script('code...')</p>\n</li>\n<li>\n<p>点击按钮：</p>\n<p>​\tbtn.click()</p>\n</li>\n<li>\n<p>后退 / 前进</p>\n<p>​\tbro.back()/forward()</p>\n</li>\n</ul>\n<h3 id=\"selenium处理iframe\"><a class=\"anchor\" href=\"#selenium处理iframe\">#</a> selenium 处理 iframe</h3>\n<p>bro.switch_to.frame('frame_name')</p>\n<h3 id=\"动作链项目暂时用不到\"><a class=\"anchor\" href=\"#动作链项目暂时用不到\">#</a> 动作链 (项目暂时用不到)</h3>\n<p>南大相关网站的登录还不需要滑块验证</p>\n<p>碰到滑块验证的时候可能就需要用到了</p>\n<h3 id=\"selenium实现模拟登录\"><a class=\"anchor\" href=\"#selenium实现模拟登录\">#</a> selenium 实现模拟登录</h3>\n<p>结合前面的基础知识 先点击账号密码登录按钮，找到账号输入框与密码输入框，</p>\n<p>输入账号密码，点击登录按钮</p>\n<h2 id=\"模拟登陆cookie操作\"><a class=\"anchor\" href=\"#模拟登陆cookie操作\">#</a> 模拟登陆 cookie 操作</h2>\n<h3 id=\"cookie有什么作用\"><a class=\"anchor\" href=\"#cookie有什么作用\">#</a> cookie 有什么作用</h3>\n<p>TCP/IP 协议是无状态的，因此通常的 get 请求 服务器端不知道你有没有处于登录状态</p>\n<p>携带 cookie 值的 get 请求 能使服务器端知道你已经处于登录状态</p>\n<h3 id=\"手动cookie不推荐\"><a class=\"anchor\" href=\"#手动cookie不推荐\">#</a> 手动 cookie（不推荐）</h3>\n<p>登录网站后通过抓包工具手动复制 cookie 值 将 cookie 值封装到 headers 中，写死在程序里</p>\n<p>缺点：cookie 值一段时间后会过期</p>\n<h3 id=\"自动处理\"><a class=\"anchor\" href=\"#自动处理\">#</a> 自动处理</h3>\n<p>cookie 值是如何产生的？既然 cookie 值能让服务器知道你已经处于登录状态，那么理所当然 cookie 值是登录时发送 post 请求后服务器端生成后发送给客户端的</p>\n<p>session 会话对象：</p>\n<p>作用：</p>\n<ol>\n<li>可以进行请求的发送</li>\n<li>如果请求过程中产生了 cookie，则该 cookie 会被自动存储 / 携带在该 session 对象中</li>\n</ol>\n<p>使用：</p>\n<ol>\n<li>\n<p>创建一个 session 对象 ：  <code>session = requests.Session()</code></p>\n</li>\n<li>\n<p>使用 session 对象进行模拟登录 post 请求的发送 (cookie 就会被存储在 session 中)</p>\n</li>\n<li>\n<p>session 对象对个人主页对应的 get 请求进行发送（携带了 cookie）</p>\n</li>\n</ol>\n<p>如果登录找不到 post 请求或遇到问题导致获得不到 cookie：</p>\n<p>​\t适用 selenium 登录后 用下列方法直接取得 cookie:</p>\n<figure class=\"highlight python\"><figcaption data-lang=\"python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre>c <span class=\"token operator\">=</span> bro<span class=\"token punctuation\">.</span>get_cookies<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre>bro<span class=\"token punctuation\">.</span>close<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre>cookies <span class=\"token operator\">=</span> <span class=\"token punctuation\">&#123;</span><span class=\"token punctuation\">&#125;</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre><span class=\"token comment\"># 获取 cookie 中的 name 和 value, 转化成 requests 可以使⽤的形式</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre><span class=\"token keyword\">for</span> cookie <span class=\"token keyword\">in</span> c<span class=\"token punctuation\">:</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre>    cookies<span class=\"token punctuation\">[</span>cookie<span class=\"token punctuation\">[</span><span class=\"token string\">'name'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> cookie<span class=\"token punctuation\">[</span><span class=\"token string\">'value'</span><span class=\"token punctuation\">]</span></pre></td></tr></table></figure><p>然后就能用 cookies 了：</p>\n<figure class=\"highlight python\"><figcaption data-lang=\"python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre>response <span class=\"token operator\">=</span> requests<span class=\"token punctuation\">.</span>get<span class=\"token punctuation\">(</span>url<span class=\"token operator\">=</span>detail_url<span class=\"token punctuation\">,</span> headers<span class=\"token operator\">=</span>headers<span class=\"token punctuation\">,</span> cookies<span class=\"token operator\">=</span>cookies<span class=\"token punctuation\">)</span></pre></td></tr></table></figure><h2 id=\"并行爬取异步爬虫\"><a class=\"anchor\" href=\"#并行爬取异步爬虫\">#</a> 并行爬取（异步爬虫）</h2>\n<p>原理：利用进程池实现并行爬取</p>\n<p>使用方法：</p>\n<figure class=\"highlight python\"><figcaption data-lang=\"python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token keyword\">from</span> multiprocessing <span class=\"token keyword\">import</span> Pool</pre></td></tr><tr><td data-num=\"2\"></td><td><pre><span class=\"token comment\"># 创建进程池</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre>pool <span class=\"token operator\">=</span> Pool<span class=\"token punctuation\">(</span>processes<span class=\"token operator\">=</span><span class=\"token number\">4</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>pool<span class=\"token punctuation\">.</span><span class=\"token builtin\">map</span><span class=\"token punctuation\">(</span>function_name<span class=\"token punctuation\">,</span> iterable<span class=\"token punctuation\">[</span><span class=\"token punctuation\">,</span>chunksize<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span></pre></td></tr></table></figure><p><strong>processes 参数设置注意</strong>：当进程数量大于 CPU 的内核数量时，等待运行的进程会等到其他进程运行完毕让出内核为止。因此，如果 CPU 是单核，就无法进行多进程并行。在使用多进程爬虫之前，我们需要先了解计算机 CPU 的核心数量。这里用到了 multiprocessing:</p>\n<figure class=\"highlight python\"><figcaption data-lang=\"python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre><span class=\"token keyword\">from</span> multiprocessing <span class=\"token keyword\">import</span> cpu_count</pre></td></tr><tr><td data-num=\"2\"></td><td><pre><span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>cpu_count<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></pre></td></tr></table></figure><p>运行结果为 16。</p>\n<p>使用 eg：</p>\n<figure class=\"highlight python\"><figcaption data-lang=\"python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre>urls <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token string\">'https://xjh.haitou.cc/xa/after/page-&#123;&#125;'</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">format</span><span class=\"token punctuation\">(</span>i<span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">21</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span></pre></td></tr><tr><td data-num=\"2\"></td><td><pre>pool <span class=\"token operator\">=</span> Pool<span class=\"token punctuation\">(</span>processes<span class=\"token operator\">=</span><span class=\"token number\">4</span><span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"3\"></td><td><pre><span class=\"token comment\">#get_xuanjing 为参数是一个 url 的爬虫函数</span></pre></td></tr><tr><td data-num=\"4\"></td><td><pre>pool<span class=\"token punctuation\">.</span><span class=\"token builtin\">map</span><span class=\"token punctuation\">(</span>get_xuanjiang<span class=\"token punctuation\">,</span>urls<span class=\"token punctuation\">)</span></pre></td></tr><tr><td data-num=\"5\"></td><td><pre>pool<span class=\"token punctuation\">.</span>close<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># 关闭进程池，不再接受新的进程</span></pre></td></tr><tr><td data-num=\"6\"></td><td><pre>pool<span class=\"token punctuation\">.</span>join<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># 主进程阻塞等待子进程的退出</span></pre></td></tr></table></figure><h2 id=\"有关爬虫的一些问题\"><a class=\"anchor\" href=\"#有关爬虫的一些问题\">#</a> 有关爬虫的一些问题：</h2>\n<ul>\n<li>\n<p>爬下来的文本中带有 &amp; nbsp 在 html 中表示 no breaking space（不自动换行的空格）</p>\n<p>此时可以采用 str.replace (u'\\xa0', '') 来解决 str 为待处理字符串</p>\n</li>\n<li>\n<p>requests 里.text 与 .content 方法的区别：</p>\n<p>两者区别在于，content 中间存的是字节码，而 text 中存的是 Beautifulsoup 根据猜测的编码方式将 content 内容编码成字符串。</p>\n<p>直接输出 content，会发现前面存在 b' 这样的标志，这是字节字符串的标志，而 text 是，没有前面的 b, 对于纯 ascii 码，这两个可以说一模一样，对于其他的文字，需要正确编码才能正常显示。大部分情况建议使用.text，因为显示的是汉字，但有时会显示乱码，这时需要用.content.decode ('utf-8')，中文常用 utf-8 和 GBK，GB2312 等。这样可以手工选择文字编码方式。</p>\n<p>所以简而言之，.text 是现成的字符串，.content 还要编码，但是.text 不是所有时候显示都正常，这是就需要用.content 进行手动编码</p>\n</li>\n<li>\n<p>两种常见方法，一般能解决所有中文乱码问题</p>\n<ol>\n<li>\n<p>对爬下来的整个数据改变编码：</p>\n<figure class=\"highlight python\"><figcaption data-lang=\"python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre>response<span class=\"token punctuation\">.</span>encoding <span class=\"token operator\">=</span> <span class=\"token string\">'utf-8'</span></pre></td></tr></table></figure></li>\n<li>\n<p>对出现问题的对象单独进行编码与解码</p>\n<figure class=\"highlight python\"><figcaption data-lang=\"python\"></figcaption><table><tr><td data-num=\"1\"></td><td><pre>img_name <span class=\"token operator\">=</span> img_name<span class=\"token punctuation\">.</span>encode<span class=\"token punctuation\">(</span><span class=\"token string\">'iso-8859-1'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>decode<span class=\"token punctuation\">(</span><span class=\"token string\">'gbk'</span><span class=\"token punctuation\">)</span></pre></td></tr></table></figure></li>\n</ol>\n</li>\n</ul>\n",
            "tags": [
                "python",
                "爬虫"
            ]
        }
    ]
}